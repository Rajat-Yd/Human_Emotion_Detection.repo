{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQCIejSbjrHygEb8MidTYo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import sklearn### machine learning library\n",
        "import cv2## image processing\n",
        "from sklearn.metrics import confusion_matrix, roc_curve### metrics\n",
        "import seaborn as sns### visualizations\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.cm as cm\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n",
        "                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n",
        "                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n",
        "                                     RandomContrast, Rescaling, Resizing, Reshape)\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
        "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
        "from tensorflow.keras.regularizers  import L2, L1\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Example, Features, Feature\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "wXzyUgsp7RYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Dataset Downloading</h1>"
      ],
      "metadata": {
        "id": "azLG64fn5I6k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60YfcTOz2o7a"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "y2Nd8S8R4pJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /content/kaggle.json"
      ],
      "metadata": {
        "id": "Rlnecu1r43kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d muhammadhananasghar/human-emotions-datasethes"
      ],
      "metadata": {
        "id": "WTzP_ebz5CpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/human-emotions-datasethes.zip\" -d \"/content/dataset\""
      ],
      "metadata": {
        "id": "93wYidPZ5Xc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_directory = \"/content/dataset/Emotions Dataset/Emotions Dataset/train\"\n",
        "val_directory =\"/content/dataset/Emotions Dataset/Emotions Dataset/test\"\n",
        "CLASS_NAMES = [\"angry\", \"happy\", \"sad\"]"
      ],
      "metadata": {
        "id": "gKXCVq6m6qzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIGURATION = {\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"IM_SIZE\": 256,\n",
        "    \"LEARNING_RATE\": 1e-3,\n",
        "    \"N_EPOCHS\": 20,\n",
        "    \"DROPOUT_RATE\": 0.0,\n",
        "    \"REGULARIZATION_RATE\": 0.0,\n",
        "    \"N_FILTERS\": 6,\n",
        "    \"KERNEL_SIZE\": 3,\n",
        "    \"N_STRIDES\": 1,\n",
        "    \"POOL_SIZE\": 2,\n",
        "    \"N_DENSE_1\": 1024,\n",
        "    \"N_DENSE_2\": 128,\n",
        "    \"NUM_CLASSES\": 3,\n",
        "    \"PATCH_SIZE\": 16,\n",
        "    \"PROJ_DIM\": 768,\n",
        "    \"CLASS_NAMES\": [\"angry\", \"happy\", \"sad\"],\n",
        "}"
      ],
      "metadata": {
        "id": "rc4BTR6jyexE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Dataset Loading</h1>"
      ],
      "metadata": {
        "id": "7n5ELGAC6LXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_directory,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=CLASS_NAMES,\n",
        "    color_mode='rgb',\n",
        "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
        "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    shuffle=True,\n",
        "    seed=99,\n",
        ")"
      ],
      "metadata": {
        "id": "wUPeS2Sk5ogP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_directory,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=CLASS_NAMES,\n",
        "    color_mode='rgb',\n",
        "    batch_size=CONFIGURATION[\"BATCH_SIZE\"],\n",
        "    image_size=(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "    shuffle=True,\n",
        "    seed=99,\n",
        ")"
      ],
      "metadata": {
        "id": "t2AUD_IRmcUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in val_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "hzlMgr2Nm7nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Data Visualization</h1>"
      ],
      "metadata": {
        "id": "mCYiAwqToo7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,10))\n",
        "\n",
        "for images, labels in train_dataset.take(1):\n",
        "  for i in range(10):\n",
        "    ax = plt.subplot(4,4, i+1)\n",
        "    plt.imshow(images[i]/255.)\n",
        "    plt.title(CLASS_NAMES[tf.argmax(labels[i], axis = 0).numpy()])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "MV3PZO0ZoCh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>DATA PREPARATION</h1>"
      ],
      "metadata": {
        "id": "xGqEMazOtyFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = (\n",
        "    train_dataset\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "CFUIvV-3phHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = (\n",
        "    train_dataset\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "XVpYaAytu9I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize_rescale_layers = tf.keras.Sequential(\n",
        "    [\n",
        "        Resizing(CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"]),\n",
        "        Rescaling(1./255),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "P8dFRY3PvHHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>MODELING</h1>"
      ],
      "metadata": {
        "id": "gr7lDV-qwzCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model = tf.keras.Sequential(\n",
        "    [\n",
        "    InputLayer(input_shape = (CONFIGURATION[\"IM_SIZE\"], CONFIGURATION[\"IM_SIZE\"], 3), ),\n",
        "\n",
        "    Rescaling(1./255, name = \"rescaling\"),\n",
        "\n",
        "    Conv2D(filters = CONFIGURATION[\"N_FILTERS\"] , kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides = CONFIGURATION[\"N_STRIDES\"] , padding='valid',\n",
        "          activation = 'relu',kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = CONFIGURATION[\"POOL_SIZE\"], strides= CONFIGURATION[\"N_STRIDES\"]*2),\n",
        "    Dropout(rate = CONFIGURATION[\"DROPOUT_RATE\"] ),\n",
        "\n",
        "    Conv2D(filters = CONFIGURATION[\"N_FILTERS\"]*2 + 4, kernel_size = CONFIGURATION[\"KERNEL_SIZE\"], strides=CONFIGURATION[\"N_STRIDES\"], padding='valid',\n",
        "          activation = 'relu', kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    MaxPool2D (pool_size = CONFIGURATION[\"POOL_SIZE\"], strides= CONFIGURATION[\"N_STRIDES\"]*2),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense( CONFIGURATION[\"N_DENSE_1\"], activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "    Dropout(rate = CONFIGURATION[\"DROPOUT_RATE\"]),\n",
        "\n",
        "    Dense( CONFIGURATION['N_DENSE_2'], activation = \"relu\", kernel_regularizer = L2(CONFIGURATION[\"REGULARIZATION_RATE\"])),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Dense(CONFIGURATION[\"NUM_CLASSES\"], activation = \"softmax\"),\n",
        "\n",
        "])\n",
        "\n",
        "lenet_model.summary()"
      ],
      "metadata": {
        "id": "ar-foWumvh6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>TRAINING</h1>"
      ],
      "metadata": {
        "id": "NI1dOPEH2o_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = CategoricalCrossentropy()\n",
        "# loss_function = CrossCategoricalCrossentropy()"
      ],
      "metadata": {
        "id": "6vT9R9jAyLhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = [CategoricalAccuracy(name = \"accuracy\"), TopKCategoricalAccuracy(k = 2, name = \"top_k_accuracy\")]"
      ],
      "metadata": {
        "id": "o3FRi15p409n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenet_model.compile(\n",
        "    optimizer = Adam(learning_rate=CONFIGURATION[\"LEARNING_RATE\"]),\n",
        "    loss = loss_function,\n",
        "    metrics = metrics\n",
        ")"
      ],
      "metadata": {
        "id": "H1GMuEHu6FvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = lenet_model.fit(\n",
        "    training_dataset,\n",
        "    validation_data = validation_dataset,\n",
        "    epochs = CONFIGURATION[\"N_EPOCHS\"],\n",
        "    verbose = True,\n",
        ")"
      ],
      "metadata": {
        "id": "9N3mjks_6dqm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}